{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits import axes_grid1\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from source.plotting_utils import plot_heatmap_colorbar, plot_heatmap_histogram, visualize_participant_performance, visualize_participant_performance_multiseed"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "benchmark = True\n",
    "visualize_individual_features = False\n",
    "center_weight_plots = True\n",
    "percentage_of_last_rounds = 0.05\n",
    "\n",
    "logdir = \"../outputs/vehicle/experiment_name/\"\n",
    "dataset_name = \"Vehicle\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup of Metrics, Methods and Directories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outdir = logdir + \"eval/\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "all_runs = os.listdir(logdir)\n",
    "if \"eval\" in all_runs:\n",
    "    all_runs.remove(\"eval\")\n",
    "metrics = [\"balanced_accuracy\",\"f1\",\"precision\",\"recall\",\"pr_auc\",\"roc_auc\"]\n",
    "methods = [\"mean-eq\", \"perc-10\", \"perc-90\"]\n",
    "\n",
    "metrics_selection = [\"balanced_accuracy\", \"f1\", \"roc_auc\"]\n",
    "\n",
    "all_run_order = [\"iFedAvg\", \"APFL\", \"FedAvg\", \"Local\", \"Centralized\"]\n",
    "runs = [run for run in all_runs if run in all_run_order]\n",
    "\n",
    "run_order = [run for run in all_run_order if run in runs]\n",
    "if not benchmark:\n",
    "    runs = runs + [run for run in all_runs if run not in runs]\n",
    "    run_order = run_order + [run for run in runs if run not in run_order]\n",
    "\n",
    "print(runs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determining whether multiple seeds have been executed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    participants = os.listdir(logdir + runs[0] + \"/logs\")\n",
    "    multiseed = False\n",
    "except:\n",
    "    multiseed = True\n",
    "    path = logdir + runs[0]\n",
    "    seeds = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name)) ]\n",
    "    print(seeds)\n",
    "    participants = os.listdir(logdir + runs[0] + \"/\" + seeds[0] + \"/logs\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def perc(n):\n",
    "    def perc_(x):\n",
    "        return np.percentile(x, n)\n",
    "    perc_.__name__ = 'perc_%s' % n\n",
    "    return perc_\n",
    "\n",
    "def get_joined_df(run):\n",
    "    dfs = []\n",
    "    for p in participants:\n",
    "        dfs.append(pd.read_csv(logdir + run + \"/logs/\" + p, sep=\";\"))\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    return df\n",
    "\n",
    "def get_joined_df_multiseed(run, std=False):\n",
    "    dfs = []\n",
    "    for p in participants:\n",
    "        for s in seeds:\n",
    "            df = pd.read_csv(logdir + run + \"/\" + s + \"/logs/\" + p, sep=\";\")\n",
    "            df[\"seed\"] = s\n",
    "            dfs.append(df)\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    means = df.groupby([\"name\", \"round\"]).mean().reset_index()\n",
    "\n",
    "    if std:\n",
    "        stds = df.groupby([\"name\", \"round\"]).std().reset_index()\n",
    "        return means, stds\n",
    "\n",
    "    return means\n",
    "\n",
    "def get_aggregate_metric(run, metric=\"f1\", method=\"perc\", multiseed=False):\n",
    "    if multiseed:\n",
    "        df = get_joined_df_multiseed(run)\n",
    "    else:\n",
    "        df = get_joined_df(run)\n",
    "    if method == \"mean-eq\":\n",
    "        return df.groupby(\"round\").mean().reset_index()[metric]\n",
    "    elif method == \"mean-smpl\":\n",
    "        weighted_metric = pd.DataFrame({\"round\": df[\"round\"], metric: df[metric] * df[\"n_samples\"]}).groupby(\"round\").sum().reset_index()[metric]\n",
    "        sample_vec = df.groupby(\"round\").sum()[\"n_samples\"]\n",
    "        return weighted_metric / sample_vec\n",
    "    elif method == \"median\":\n",
    "        return df.groupby(\"round\").agg({metric: perc(50)})[metric]\n",
    "    elif method == \"perc-10\":\n",
    "        return df.groupby(\"round\").agg({metric: perc(10)})[metric]\n",
    "    elif method == \"perc-90\":\n",
    "        return df.groupby(\"round\").agg({metric: perc(90)})[metric]\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "\n",
    "def get_metrics_df(metric=\"f1\", method=\"mean-eq\", multiseed=False):\n",
    "    df = pd.DataFrame()\n",
    "    for run in runs:\n",
    "        df[run] = get_aggregate_metric(run, metric, method, multiseed)\n",
    "    df.index = df.index.set_names([\"round\"])\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def make_table(data, metric, method):\n",
    "    ending = data.iloc[-int(percentage_of_last_rounds*len(data)):][runs]\n",
    "    col_name = str(method + \"-\" + metric)\n",
    "    df = pd.DataFrame({col_name + \"-avg\":ending.mean(), col_name + \"-std\":ending.std()})\n",
    "    return df\n",
    "\n",
    "def run_eval(metric=\"f1\", method=\"mean-eq\", multiseed=False):\n",
    "    \"\"\"\n",
    "    Runs an evaluation for a particular metric, with a method (such as mean or 10th percentile)\n",
    "\n",
    "    Outputs (on disk) the visualizations and returns the partial DataFrame\n",
    "    \"\"\"\n",
    "    title = method + \"-\" + metric\n",
    "    data = get_metrics_df(metric, method, multiseed)\n",
    "    plot_means(data, title)\n",
    "    plot_boxplot(data, title)\n",
    "\n",
    "    df_results = make_table(data, metric, method)\n",
    "    return df_results\n",
    "\n",
    "def save_tables(table):\n",
    "    \"\"\"\n",
    "    Saves a full table and subset of results as CSV files\n",
    "    \"\"\"\n",
    "    table.to_csv(outdir + \"full_results.csv\", sep=\";\")\n",
    "\n",
    "    # create selection of the table...\n",
    "    selection_cols = [\"mean-eq-balanced_accuracy-avg\", \"mean-eq-balanced_accuracy-std\",\n",
    "                      \"perc-10-balanced_accuracy-avg\", \"perc-10-balanced_accuracy-std\",\n",
    "                      \"mean-eq-roc_auc-avg\", \"mean-eq-roc_auc-std\",\n",
    "                      \"perc-10-roc_auc-avg\", \"perc-10-roc_auc-std\",\n",
    "                      \"mean-eq-f1-avg\"]\n",
    "    new_names = [\"mean-BA-avg\", \"mean-BA-std\",\n",
    "                 \"10thPer-BA-avg\", \"10thPer-BA-std\",\n",
    "                 \"mean-ROCAUC-avg\", \"mean-ROCAUC-std\",\n",
    "                 \"10thPer-ROCAUC-avg\", \"10thPer-ROCAUC-std\",\n",
    "                 \"mean-F1-avg\"]\n",
    "    subset = table[selection_cols].round(3)\n",
    "    subset.columns = new_names\n",
    "    subset.to_csv(outdir + \"subset_results.csv\", sep=\";\")\n",
    "\n",
    "def compile_participant_table():\n",
    "    \"\"\"\n",
    "    creates a single DF with data for each participant\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for run in runs:\n",
    "        if multiseed:\n",
    "            df_2 = get_joined_df_multiseed(run)\n",
    "        else:\n",
    "            df_2 = get_joined_df(run)\n",
    "\n",
    "        last_df = df_2[df_2[\"round\"] > ((1-percentage_of_last_rounds) * df_2[\"round\"].max())]\n",
    "        last_df = last_df.groupby([\"name\"]).median().reset_index()\n",
    "        last_df.drop([\"round\", \"n_samples\"], axis=1, inplace=True)\n",
    "        last_df[\"Method\"] = run\n",
    "        df = pd.concat([df, last_df], axis=0)\n",
    "\n",
    "    # save tables\n",
    "    df = df.round(3)\n",
    "    df.to_csv(outdir + \"participant_results.csv\", sep=\";\", index=False)\n",
    "    return df\n",
    "\n",
    "def get_multiseed_means_stds():\n",
    "    all_means = pd.DataFrame()\n",
    "    all_stds = pd.DataFrame()\n",
    "\n",
    "    # create individual output tables\n",
    "    for run in runs:\n",
    "        means, stds = get_joined_df_multiseed(run, std=True)\n",
    "\n",
    "        means_end = means[means[\"round\"] > ((1-percentage_of_last_rounds) * means[\"round\"].max())]\n",
    "        means_end = means_end.groupby([\"name\"]).median().reset_index()\n",
    "        means_end.drop([\"round\", \"n_samples\"], axis=1, inplace=True)\n",
    "        means_end[\"Method\"] = run\n",
    "        all_means = pd.concat([all_means, means_end], axis=0)\n",
    "\n",
    "        stds_end = stds[stds[\"round\"] > ((1-percentage_of_last_rounds) * stds[\"round\"].max())]\n",
    "        stds_end = stds_end.groupby([\"name\"]).median().reset_index()\n",
    "        stds_end.drop([\"round\", \"n_samples\"], axis=1, inplace=True)\n",
    "        stds_end[\"Method\"] = run\n",
    "        all_stds = pd.concat([all_stds, stds_end], axis=0)\n",
    "\n",
    "    return all_means, all_stds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Individual Plotting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_means(data, title):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    melted = data.melt(id_vars=[\"round\"], var_name=\"Method\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.lineplot(data=melted, x=\"round\",y=\"value\", hue=\"Method\", hue_order=run_order)\n",
    "    plt.title(str(title))\n",
    "    plt.savefig(outdir + \"line-\" + title + \".jpg\", dpi=300)\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plot_boxplot(data, title):\n",
    "    ending = data.iloc[-int(percentage_of_last_rounds*len(data)):][runs]\n",
    "    melted = ending.melt(var_name=\"Method\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.boxplot(y=\"value\", x=\"Method\", data=melted, order=run_order, showfliers=False, palette=\"pastel\")\n",
    "    if len(list(ax.get_xticklabels())) > 5:\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "    plt.title(str(\"Metric: \"+ title))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir + \"box-\" +  title + \".jpg\", dpi=300)\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running Post-Processing Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"multiseed:\", multiseed)\n",
    "\n",
    "# Collect full results table\n",
    "table = pd.DataFrame()\n",
    "for metric, method in itertools.product(metrics_selection, methods):\n",
    "    chunk = run_eval(metric, method, multiseed=multiseed)\n",
    "    table = pd.concat([table, chunk], axis=1)\n",
    "\n",
    "save_tables(table)\n",
    "\n",
    "participant_table = compile_participant_table()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prints violin plots for each metric\n",
    "for metric in metrics_selection:\n",
    "    p_subset = participant_table[[\"Method\", \"name\", metric]]\n",
    "    visualize_participant_performance(df=p_subset, metric=metric, outdir=outdir, run_order=run_order)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creates the violin plots with error bars\n",
    "if multiseed:\n",
    "    all_means, all_stds = get_multiseed_means_stds()\n",
    "\n",
    "    for metric in metrics_selection:\n",
    "        means_subset = all_means[[\"Method\", metric]]\n",
    "        stds_subset = all_stds[[\"Method\", metric]]\n",
    "        visualize_participant_performance_multiseed(dataset_name, df_means=means_subset, df_stds=stds_subset, metric=metric, outdir=outdir, run_order=run_order)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}